{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Cancer Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "<p>A lot has been said during the past several years about how precision medicine and, more concretely, how genetic testing is going to disrupt the way diseases like cancer are treated.</p>\n",
    "<p>Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers).  As not all mutations lead to cancer.</p>\n",
    "<p>Due to a mutation in a gene, there is some genetic variation developed in a gene. But the question comes, from which particular mutation this genetic variation happened in a gene. Currently this interpretation of genetic mutations is being done manually which is a very time-consuming task. Based on a gene and a variation in it, a clinical pathologist has to manually review and classify every single genetic mutation based on evidence from text-based clinical literature.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "<p>The workflow is as follows:</p>\n",
    "<p>1. A molecular pathologist selects a list of genetic variations of interest that he/she want to analyze.</p>\n",
    "<p>2. The molecular pathologist searches for evidence in the medical literature that somehow are relevant to the genetic variations of interest.</p>\n",
    "<p>3. Finally, this molecular pathologist spends a huge amount of time analyzing the evidence related to each of the variations to classify them into any one of the 9 different classes.</p>\n",
    "<p>Our goal here is to replace step 3 by a machine learning model. The molecular pathologist will still have to decide which variations are of interest, and also collect the relevant evidence for them. But the last step, which is also the most time consuming, will be fully automated by a machine learning model.</p>\n",
    "\n",
    "## Problem Statement\n",
    "<p>Classify the given genetic variations/mutations based on evidence from text-based clinical literature. In this problem, we need to find the mutation-type given the gene, variation and some text data from published research.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source of Data\n",
    "<p> Source: https://www.kaggle.com/c/msk-redefining-cancer-treatment/data</p>\n",
    "<p> Data: Memorial Sloan Kettering Cancer Center (MSKCC)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world/Business objectives and constraints.\n",
    "* No low-latency requirement.\n",
    "* Interpretability is important.\n",
    "* Errors can be very costly.\n",
    "* Probability of a data-point belonging to each class is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Problem Formulation\n",
    "### Data Overview\n",
    "<p>- Source: https://www.kaggle.com/c/msk-redefining-cancer-treatment/data</p>\n",
    "<p>- We have two data files: one conatins the information about the genetic mutations and the other contains the clinical evidence (text) that  human experts/pathologists use to classify the genetic mutations. Both these data files are have a common column called ID.</p>\n",
    "<p>Data file's information:</p>\n",
    "    <ul> \n",
    "        <li>\n",
    "        training_variants (ID , Gene, Variations, Class)\n",
    "        </li>\n",
    "        <li>\n",
    "        training_text (ID, Text)\n",
    "        </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the real-world problem to an ML problem\n",
    "\n",
    "### Type of Machine Learning Problem\n",
    "\n",
    "<p>There are nine different classes a genetic mutation can be classified into => Multi class classification problem</p>\n",
    "\n",
    "### Performance Metric\n",
    "\n",
    "<p>Source: https://www.kaggle.com/c/msk-redefining-cancer-treatment#evaluation</p>\n",
    "\n",
    "<h4 style = \"font-size:17px\">Metric(s):</h4>\n",
    "\n",
    "<ul> \n",
    "    <li>\n",
    "        Multi class log-loss\n",
    "    </li>\n",
    "    <li>\n",
    "        Confusion matrix \n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Descriptions\n",
    "<p><b>TrainingData</b> - a comma separated file containing the description of the genetic mutations used for training. Fields are ID (the id of the row used to link the mutation to the clinical evidence), Gene (the gene where this genetic mutation is located), Variation (the aminoacid change for this mutations), Class (1-9 the class this genetic mutation has been classified on)</p>\n",
    "<p><b>TrainingText</b> - a double pipe (||) delimited file that contains the clinical evidence (text) used to classify genetic mutations. Fields are ID (the id of the row used to link the clinical evidence to the genetic mutation), Text (the clinical evidence used to classify the genetic mutation)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, CV and Test Datasets\n",
    "<p> Split the dataset randomly into three parts train, cross validation and test with 64%,16%, 20% of data respectively</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Exploratory Data Analysis</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c8b4981e89ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "warnings.simplefilter('ignore')\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Reading Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/TrainingData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = pd.read_csv(\"../Data/TrainingText\", sep = \"\\|\\|\", names = [\"ID\", \"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data_text.drop(0, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data_text.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data_text.drop([\"ID\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data = pd.concat([data, data_text], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data_Label = Final_Data[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data = Final_Data.drop([\"Class\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data = pd.concat([Final_Data, Final_Data_Label], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for rows having text column as nan\n",
    "for index, textinfo in Final_Data.iterrows():\n",
    "    if type(textinfo[\"Text\"]) is not str:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing rows with text data as nan\n",
    "for index, textinfo in Final_Data.iterrows():\n",
    "    if type(textinfo[\"Text\"]) is not str:\n",
    "        Final_Data.drop(index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(clinicalText, indexNumber, columnName):\n",
    "    if type(clinicalText) is not int:\n",
    "        textString = \"\"\n",
    "        #removing special characters\n",
    "        clinicalText1 = re.sub('[^a-zA-Z0-9\\n]', ' ', clinicalText)\n",
    "        #remove any spaces\n",
    "        clinicalText2 = re.sub('\\s+', ' ', clinicalText1)\n",
    "        #converting text to lowercase\n",
    "        textString += clinicalText2.lower()\n",
    "        Final_Data[columnName][indexNumber] = textString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.clock() # this function returns wall-clock seconds elapsed since the first call to this function as float\n",
    "for index, textinfo in Final_Data.iterrows():\n",
    "    pre_processing(textinfo[\"Text\"], index, \"Text\")\n",
    "print(\"Total time for Preprocessing the text data = \"+str(time.clock() - startTime)+\"Sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data_Labels = Final_Data[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data.shape, Final_Data_Labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train, cross-validation and test split</h1>\n",
    "<p> Split the dataset randomly into three parts train, cross validation and test with 64%,16%, 20% of data respectively.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing any space with underscore\n",
    "Final_Data[\"Gene\"] = Final_Data[\"Gene\"].str.replace(\"\\s+\", \"_\")\n",
    "Final_Data[\"Variation\"] = Final_Data[\"Variation\"].str.replace(\"\\s+\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, TestData, Y_Train, TestLabels = train_test_split(Final_Data, Final_Data_Labels, stratify=Final_Data_Labels, test_size=0.2)\n",
    "TrainData, CV_Data, TrainDataLabels, CV_Data_Labels = train_test_split(X_Train, Y_Train, stratify=Y_Train, test_size=0.2)\n",
    "\n",
    "# This stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the \n",
    "# proportion of values provided to parameter stratify.\n",
    "# For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, \n",
    "# stratify=y will make sure that your random split has 25% of 0's and 75% of 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train.shape, TestData.shape, Y_Train.shape, TestLabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of data points in train data:', TrainData.shape[0])\n",
    "print('Number of data points in test data:', TestData.shape[0])\n",
    "print('Number of data points in cross validation data:', CV_Data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Distribution of Class labels in Train, CV and Test data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData_distribution = TrainData[\"Class\"].value_counts().sort_index()\n",
    "CVData_distribution = CV_Data[\"Class\"].value_counts().sort_index()\n",
    "TestData_distribution = TestData[\"Class\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData_distribution_sorted = sorted(TrainData_distribution.items(), key = lambda d: d[1], reverse = True)\n",
    "CVData_distribution_sorted = sorted(CVData_distribution.items(), key = lambda d: d[1], reverse = True)\n",
    "TestData_distribution_sorted = sorted(TestData_distribution.items(), key = lambda d: d[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "TrainData_distribution.plot(kind = \"bar\")\n",
    "plt.grid()\n",
    "plt.title(\"Distribution of class labels in training data\", fontsize = 20)\n",
    "plt.xlabel(\"Class\", fontsize = 20)\n",
    "plt.ylabel(\"Number of Data Points\", fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "for i in TrainData_distribution_sorted:\n",
    "    print(\"Number of training data points in class \"+str(i[0])+\" = \"+str(i[1])+ \"(\"+str(np.round(((i[1]/TrainData.shape[0])*100), 4))+\"%)\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "CVData_distribution.plot(kind = \"bar\")\n",
    "plt.grid()\n",
    "plt.title(\"Distribution of class labels in validation data\", fontsize = 20)\n",
    "plt.xlabel(\"Class\", fontsize = 20)\n",
    "plt.ylabel(\"Number of Data Points\", fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "for i in CVData_distribution_sorted:\n",
    "    print(\"Number of CV data points in class \"+str(i[0])+\" = \"+str(i[1])+ \"(\"+str(np.round(((i[1]/CV_Data.shape[0])*100), 4))+\"%)\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "TestData_distribution.plot(kind = \"bar\")\n",
    "plt.grid()\n",
    "plt.title(\"Distribution of class labels in test data\", fontsize = 20)\n",
    "plt.xlabel(\"Class\", fontsize = 20)\n",
    "plt.ylabel(\"Number of Data Points\", fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "for i in TestData_distribution_sorted:\n",
    "    print(\"Number of test data points in class \"+str(i[0])+\" = \"+str(i[1])+ \"(\"+str(np.round(((i[1]/TestData.shape[0])*100), 4))+\"%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prediction using a 'Random' Model</h1>\n",
    "<p>In a 'Random' Model, we generate the NINE class probabilites randomly such that they sum to 1.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusionMatrix(Y_TestLabels, PredictedLabels):\n",
    "    confusionMatx = confusion_matrix(Y_TestLabels, PredictedLabels)\n",
    "    \n",
    "    precision = confusionMatx/confusionMatx.sum(axis = 0)\n",
    "    \n",
    "    recall = (confusionMatx.T/confusionMatx.sum(axis = 1)).T\n",
    "    \n",
    "    # confusionMatx = [[1, 2],\n",
    "    #                  [3, 4]]\n",
    "    # confusionMatx.T = [[1, 3],\n",
    "    #                   [2, 4]]\n",
    "    # confusionMatx.sum(axis = 1)  axis=0 corresponds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "    # confusionMatx.sum(axix =1) = [[3, 7]]\n",
    "    # (confusionMatx.T)/(confusionMatx.sum(axis=1)) = [[1/3, 3/7]\n",
    "    #                                                  [2/3, 4/7]]\n",
    "\n",
    "    # (confusionMatx.T)/(confusionMatx.sum(axis=1)).T = [[1/3, 2/3]\n",
    "    #                                                    [3/7, 4/7]]\n",
    "    # sum of row elements = 1\n",
    "    \n",
    "    labels = [i for i in range(1, 10)]\n",
    "    \n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.heatmap(confusionMatx, cmap = \"Reds\", annot = True, xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Confusion Matrix\", fontsize = 30)\n",
    "    plt.xlabel('Predicted Class', fontsize = 20)\n",
    "    plt.ylabel('Original Class', fontsize = 20)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"-\"*125)\n",
    "    \n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.heatmap(precision, cmap = \"Reds\", annot = True, fmt = \".4f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Precision Matrix\", fontsize = 30)\n",
    "    plt.xlabel('Predicted Class', fontsize = 20)\n",
    "    plt.ylabel('Original Class', fontsize = 20)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"-\"*125)\n",
    "    \n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.heatmap(recall, cmap = \"Reds\", annot = True, fmt = \".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Recall Matrix\", fontsize = 30)\n",
    "    plt.xlabel('Predicted Class', fontsize = 20)\n",
    "    plt.ylabel('Original Class', fontsize = 20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataLength = TestData.shape[0]\n",
    "CVDataLength = CV_Data.shape[0]\n",
    "\n",
    "CV_predicted_probs = np.zeros((CVDataLength,9))\n",
    "for i in range(CVDataLength):\n",
    "    rand_probs_CV = np.random.rand(1,9)  #it will return an array of random numbers between 1 and 0 of size 1*9\n",
    "    CV_predicted_probs[i] = (rand_probs_CV/sum(sum(rand_probs_CV)))[0]  #it will generate random probabilities of each point in CV data such that theor sum = 1\n",
    "print(\"Log loss on Cross Validation Data using Random Model \"+str(log_loss(CV_Data_Labels,CV_predicted_probs)))\n",
    "\n",
    "test_predicted_probs = np.zeros((testDataLength,9))\n",
    "for i in range(testDataLength):\n",
    "    rand_probs_test = np.random.rand(1,9)  #it will return an array of random numbers between 1 and 0 of size 1*9\n",
    "    test_predicted_probs[i] = (rand_probs_test/sum(sum(rand_probs_test)))[0]  #it will generate random probabilities of each point in CV data such that theor sum = 1\n",
    "print(\"Log loss on Test Data using Random Model \"+str(log_loss(TestLabels, test_predicted_probs)))\n",
    "\n",
    "pred_test_labels = np.argmax(test_predicted_probs, axis = 1)  #here, axis 1 means row wise\n",
    "print_confusionMatrix(TestLabels, pred_test_labels+1)  #here pred_test_labels+1 plus 1 because argmax counts from 0 but our class labels are from 1 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_coded_featureDict(alpha, feature, df):\n",
    "    \n",
    "    value_count = TrainData[feature].value_counts()\n",
    "    rc_values = dict()\n",
    "    \n",
    "    for i, denominator in value_count.items():\n",
    "        vec = []\n",
    "        for k in range(1, 10):\n",
    "            cls_cnt = TrainData.loc[(TrainData[\"Class\"]==k) & (TrainData[feature]==i)]\n",
    "# build a vector (1*9) , the first element = (number of times it occured in class1 + 10*alpha / number of time it occurred in total data+(90*alpha))\n",
    "# here, adding 10*alpha in numerator and 90*alpha in denominator is nothing but laplace smoothing which we did in Naive Bayes \n",
    "            vec.append((cls_cnt.shape[0] + 10*alpha)/denominator + (90*alpha))    \n",
    "        rc_values[i] = vec\n",
    "    return rc_values\n",
    "\n",
    "\n",
    "def response_coded_values(alpha, feature, df):\n",
    "    get_rc_values = response_coded_featureDict(alpha, feature, df)\n",
    "    get_value_count = TrainData[feature].value_counts()\n",
    "    get_feat_values = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row[feature] in dict(get_value_count).keys():\n",
    "            get_feat_values.append(get_rc_values[row[feature]])\n",
    "        else:\n",
    "            get_feat_values.append([1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9])\n",
    "    return get_feat_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we calculate the probability of a feature belongs to any particular class, we apply laplace smoothing\n",
    "<li>(numerator + alpha) / (denominator + (9*alpha)) </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Univariate Analysis on Gene Feature</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>Q1.</b> Gene, What type of feature it is ?</p>\n",
    "<p><b>Ans.</b> Gene is a categorical variable </p>\n",
    "<p> <b>Q2.</b> How many categories are there and how they are distributed?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genes = TrainData[\"Gene\"].value_counts()\n",
    "print(\"Number of Unique genes = \"+str(unique_genes.shape[0]))\n",
    "print(unique_genes.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ans: There are\", unique_genes.shape[0] ,\"different categories of genes in the train data, and they are distibuted as follows:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sum(unique_genes.values)\n",
    "h = unique_genes.values/s\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(h)\n",
    "plt.title(\"Histrogram of Genes\", fontsize = 20)\n",
    "plt.xlabel('Index of a Gene', fontsize = 20)\n",
    "plt.ylabel('Number of Occurances', fontsize = 20)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sum(unique_genes.values)\n",
    "h = unique_genes.values/s\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(np.cumsum(h))\n",
    "plt.title(\"Cumulative plot of Genes\", fontsize = 20)\n",
    "plt.xlabel('Index of a Gene', fontsize = 20)\n",
    "plt.ylabel('Percentage of Occurances', fontsize = 20)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>Q3.</b> How to featurize this Gene feature ?</p>\n",
    "<ol><li>Response Coding</li><li>One-hot Coding</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response-coding of the Gene feature\n",
    "# alpha is used for laplace smoothing\n",
    "alpha = 1\n",
    "# train gene feature\n",
    "train_gene_feature_responseCoding = np.array(response_coded_values(alpha, \"Gene\", TrainData))\n",
    "# test gene feature\n",
    "test_gene_feature_responseCoding = np.array(response_coded_values(alpha, \"Gene\", TestData))\n",
    "# cross validation gene feature\n",
    "cv_gene_feature_responseCoding = np.array(response_coded_values(alpha, \"Gene\", CV_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert each row values such that they sum to 1  \n",
    "train_gene_feature_responseCoding = (train_gene_feature_responseCoding.T/train_gene_feature_responseCoding.sum(axis=1)).T\n",
    "test_gene_feature_responseCoding = (test_gene_feature_responseCoding.T/test_gene_feature_responseCoding.sum(axis=1)).T\n",
    "cv_gene_feature_responseCoding = (cv_gene_feature_responseCoding.T/cv_gene_feature_responseCoding.sum(axis=1)).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of response encoded features in train data = \"+str(train_gene_feature_responseCoding.shape))\n",
    "print(\"Size of response encoded features in test data = \"+str(test_gene_feature_responseCoding.shape))\n",
    "print(\"Size of response encoded features in CV data = \"+str(cv_gene_feature_responseCoding.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneVectorizer = CountVectorizer()\n",
    "train_gene_feature_onehotCoding = geneVectorizer.fit_transform(TrainData['Gene'])\n",
    "test_gene_feature_onehotCoding = geneVectorizer.transform(TestData['Gene'])\n",
    "cv_gene_feature_onehotCoding = geneVectorizer.transform(CV_Data['Gene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of one-hot encoded features in train data = \"+str(train_gene_feature_onehotCoding.shape))\n",
    "print(\"Size of one-hot encoded features in test data = \"+str(test_gene_feature_onehotCoding.shape))\n",
    "print(\"Size of one-hot encoded features in CV data = \"+str(cv_gene_feature_onehotCoding.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q4. How good is this gene feature in predicting y_i?</h3>\n",
    "<p>There are many ways to estimate how good a feature is, in predicting y_i. One of the good methods is to build a proper ML model using just this feature. In this case, we will build a logistic regression model using only Gene feature (one hot encoded) to predict y_i.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 1)]\n",
    "\n",
    "cv_log_loss = []\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_gene_feature_onehotCoding, TrainDataLabels)\n",
    "    calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "    #this is a cross-validation\n",
    "    calib_clf.fit(train_gene_feature_onehotCoding, TrainDataLabels)\n",
    "    predicted_y = calib_clf.predict_proba(cv_gene_feature_onehotCoding)\n",
    "    cv_log_loss.append(log_loss(CV_Data_Labels, predicted_y, labels=clf.classes_))\n",
    "    print(\"For alpha value of \"+str(i)+\" CV log loss = \"+str(log_loss(CV_Data_Labels, predicted_y, labels=clf.classes_)))\n",
    "\n",
    "plt.figure(figsize = (12, 7))\n",
    "plt.plot(alpha, cv_log_loss)\n",
    "for xy in zip(alpha, np.round(cv_log_loss, 4)):\n",
    "    plt.annotate(xy, xy)\n",
    "    \n",
    "plt.title(\"Alpha vs Log-loss\", fontsize = 20)\n",
    "plt.xlabel(\"Alpha\", fontsize = 20)\n",
    "plt.ylabel(\"Log-Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alpha[np.argmin(cv_log_loss)]\n",
    "clf = SGDClassifier(alpha=best_alpha, penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_gene_feature_onehotCoding, TrainDataLabels)\n",
    "calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "calib_clf.fit(train_gene_feature_onehotCoding, TrainDataLabels)\n",
    "\n",
    "predict_train = calib_clf.predict_proba(train_gene_feature_onehotCoding)\n",
    "print('For values of best alpha = ', best_alpha, \"the train log loss =:\",log_loss(TrainDataLabels, predict_train, labels=clf.classes_))\n",
    "\n",
    "predict_cv = calib_clf.predict_proba(cv_gene_feature_onehotCoding)\n",
    "print('For values of best alpha = ', best_alpha, \"the CV log loss =:\",log_loss(CV_Data_Labels, predict_cv, labels=clf.classes_))\n",
    "\n",
    "predict_test = calib_clf.predict_proba(test_gene_feature_onehotCoding)\n",
    "print('For values of best alpha = ', best_alpha, \"the test log loss =:\",log_loss(TestLabels, predict_test, labels=clf.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q5. Is the Gene feature stable across all the data sets (Test, Train, Cross validation)?</h3>\n",
    "<p> <b>Ans:</b> Yes, it is. Otherwise, the CV and Test errors would be significantly more than train error.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ques: How many common gene features are there in train, cv and test data?\")\n",
    "gene_common_in_test = TestData[TestData[\"Gene\"].isin(TrainData[\"Gene\"])].shape[0]\n",
    "gene_common_in_cv = CV_Data[CV_Data[\"Gene\"].isin(TrainData[\"Gene\"])].shape[0]\n",
    "print(\"Ans:\")\n",
    "print(\"Percentage of common gene features in test and train data = \"+str(np.round((gene_common_in_test/TestData.shape[0])*100, 2))+\"%\")\n",
    "#it prints: Out of total features in test data, how many features are also present in train data\n",
    "print(\"Percentage of common gene features in CV and train data = \"+str(np.round((gene_common_in_cv/CV_Data.shape[0])*100, 2))+\"%\")\n",
    "#it prints: Out of total features in cv data, how many features are also present in train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Univariate Analysis on Variation Feature</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Q1. Variation, What type of feature is it?</p>\n",
    "<p><b>Ans.</b> Variation is a categorical variable</p>\n",
    "<p>Q2. How many categories are there?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_variation = TrainData[\"Variation\"].value_counts()\n",
    "print(\"Number of Unique variations = \"+str(unique_variation.shape[0]))\n",
    "print(unique_variation.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ans: There are\", unique_variation.shape[0] ,\"different categories of variations in the train data, and they are distibuted as follows:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sum(unique_variation.values)\n",
    "h = unique_variation.values/s\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(h)\n",
    "plt.title(\"Histrogram of Variations\", fontsize = 20)\n",
    "plt.xlabel('Index of a Variation', fontsize = 20)\n",
    "plt.ylabel('Number of Occurances', fontsize = 20)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sum(unique_variation.values)\n",
    "h = unique_variation.values/s\n",
    "c = np.cumsum(h)\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(c)\n",
    "plt.title(\"Cumulative plot of Variations\", fontsize = 20)\n",
    "plt.xlabel('Index of a Variation', fontsize = 20)\n",
    "plt.ylabel('Percentage of Occurrences', fontsize = 20)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>Q3.</b> How to featurize this Variation feature ?</p>\n",
    "<ol><li>Response Coding</li><li>One-hot Coding</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response-coding of the variation feature\n",
    "# alpha is used for laplace smoothing\n",
    "alpha = 1\n",
    "# train variation feature\n",
    "train_variation_feature_responseCoding = np.array(response_coded_values(alpha, \"Variation\", TrainData))\n",
    "# test variation feature\n",
    "test_variation_feature_responseCoding = np.array(response_coded_values(alpha, \"Variation\", TestData))\n",
    "# cross validation variation feature\n",
    "cv_variation_feature_responseCoding = np.array(response_coded_values(alpha, \"Variation\", CV_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert each row values such that they sum to 1  \n",
    "train_variation_feature_responseCoding = (train_variation_feature_responseCoding.T/train_variation_feature_responseCoding.sum(axis=1)).T\n",
    "test_variation_feature_responseCoding = (test_variation_feature_responseCoding.T/test_variation_feature_responseCoding.sum(axis=1)).T\n",
    "cv_variation_feature_responseCoding = (cv_variation_feature_responseCoding.T/cv_variation_feature_responseCoding.sum(axis=1)).T  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of response encoded features in train data = \"+str(train_variation_feature_responseCoding.shape))\n",
    "print(\"Size of response encoded features in test data = \"+str(test_variation_feature_responseCoding.shape))\n",
    "print(\"Size of response encoded features in CV data = \"+str(cv_variation_feature_responseCoding.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variationVectorizer = CountVectorizer()\n",
    "train_variation_feature_onehotCoding = variationVectorizer.fit_transform(TrainData['Variation'])\n",
    "test_variation_feature_onehotCoding = variationVectorizer.transform(TestData['Variation'])\n",
    "cv_variation_feature_onehotCoding = variationVectorizer.transform(CV_Data['Variation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of one-hot encoded features in train data = \"+str(train_variation_feature_onehotCoding.shape))\n",
    "print(\"Size of one-hot encoded features in test data = \"+str(test_variation_feature_onehotCoding.shape))\n",
    "print(\"Size of one-hot encoded features in CV data = \"+str(cv_variation_feature_onehotCoding.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q4. How good is this Variation feature in predicting y_i?</h3>\n",
    "<p>There are many ways to estimate how good a feature is, in predicting y_i. One of the good methods is to build a proper ML model using just this feature. In this case, we will build a logistic regression model using only Variation feature (one hot encoded) to predict y_i.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 1)]\n",
    "\n",
    "cv_log_loss = []\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_variation_feature_onehotCoding, TrainDataLabels)\n",
    "    calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "    calib_clf.fit(train_variation_feature_onehotCoding, TrainDataLabels)\n",
    "    predicted_y = calib_clf.predict_proba(cv_variation_feature_onehotCoding)\n",
    "    cv_log_loss.append(log_loss(CV_Data_Labels, predicted_y, labels=clf.classes_))\n",
    "    print(\"For alpha value of \"+str(i)+\" CV log loss = \"+str(log_loss(CV_Data_Labels, predicted_y, labels=clf.classes_)))\n",
    "\n",
    "plt.figure(figsize = (12, 7))\n",
    "plt.plot(alpha, cv_log_loss)\n",
    "for xy in zip(alpha, np.round(cv_log_loss, 4)):\n",
    "    plt.annotate(xy, xy)\n",
    "    \n",
    "plt.title(\"Alpha vs Log-loss\", fontsize = 20)\n",
    "plt.xlabel(\"Alpha\", fontsize = 20)\n",
    "plt.ylabel(\"Log-Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alpha[np.argmin(cv_log_loss)]\n",
    "clf = SGDClassifier(alpha=best_alpha, penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_variation_feature_onehotCoding, TrainDataLabels)\n",
    "calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "calib_clf.fit(train_variation_feature_onehotCoding, TrainDataLabels)\n",
    "\n",
    "predict_train = calib_clf.predict_proba(train_variation_feature_onehotCoding)\n",
    "print('For values of best alpha = ', best_alpha, \"the train log loss =:\",log_loss(TrainDataLabels, predict_train, labels=clf.classes_))\n",
    "\n",
    "predict_cv = calib_clf.predict_proba(cv_variation_feature_onehotCoding)\n",
    "print('For values of best alpha = ', best_alpha, \"the CV log loss =:\",log_loss(CV_Data_Labels, predict_cv, labels=clf.classes_))\n",
    "\n",
    "predict_test = calib_clf.predict_proba(test_variation_feature_onehotCoding)\n",
    "print('For values of best alpha = ', best_alpha, \"the test log loss =:\",log_loss(TestLabels, predict_test, labels=clf.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q5. Is the Variation feature stable across all the data sets (Test, Train, Cross validation)?</h3>\n",
    "<p> <b>Ans:</b> Not sure, let's check it out.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ques: How many common variation features are there in train, cv and test data?\")\n",
    "variation_common_in_test = TestData[TestData[\"Variation\"].isin(TrainData[\"Variation\"])].shape[0]\n",
    "variation_common_in_cv = CV_Data[CV_Data[\"Variation\"].isin(TrainData[\"Variation\"])].shape[0]\n",
    "print(\"Ans:\")\n",
    "print(\"Percentage of common Variation features in test and train data = \"+str(np.round((variation_common_in_test/TestData.shape[0])*100, 2))+\"%\")\n",
    "#it prints: Out of total features in test data, how many features are also present in train data\n",
    "print(\"Percentage of common Variation features in CV and train data = \"+str(np.round((variation_common_in_cv/CV_Data.shape[0])*100, 2))+\"%\")\n",
    "#it prints: Out of total features in cv data, how many features are also present in train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Univariate Analysis on Text Feature</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many unique words are present in train data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt = defaultdict(int)\n",
    "for index, row in TrainData.iterrows():\n",
    "    for word in row['Text'].split():\n",
    "        dictt[word] += 1\n",
    "print(\"Number of unique words in train data = \"+str(len(dictt.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>How to featurize text field?</p>\n",
    "<ol>\n",
    "    <li>Response Coding</li>\n",
    "    <li>TFIDF</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Response Coding</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextWordOccurrenceDict(cls_text):\n",
    "    dictt = defaultdict(int)\n",
    "    for index, row in cls_text.iterrows():\n",
    "        for word in row['Text'].split():\n",
    "            dictt[word] += 1\n",
    "    return dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "for i in range(1,10):\n",
    "    cls_text = TrainData[TrainData['Class']==i]\n",
    "    # build a word dict based on the words in that class\n",
    "    dict_list.append(TextWordOccurrenceDict(cls_text))\n",
    "    # append it to dict_list\n",
    "# dict_list[i] is build on i'th  class text data\n",
    "# total_dict is buid on whole training text data\n",
    "total_dict = TextWordOccurrenceDict(TrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createResponseCodedText(df):\n",
    "    alpha = 10\n",
    "    textFeatueResponseCode = np.zeros((df.shape[0], 9))\n",
    "    for i in range(0,9):\n",
    "        rowIndex = 0\n",
    "        for index, rows in df.iterrows():\n",
    "            sumProbability = 0\n",
    "            for word in row[\"Text\"].split():\n",
    "                sumProbability += math.log(((dict_list[i].get(word,0))+alpha)/((total_dict.get(word,0))+(9*alpha)))\n",
    "            textFeatueResponseCode[rowIndex][i] = math.exp(sumProbability/len(row[\"Text\"].split()))\n",
    "            rowIndex += 1\n",
    "    return textFeatueResponseCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response coding of text features\n",
    "train_text_feature_responseCoding  = createResponseCodedText(TrainData)\n",
    "test_text_feature_responseCoding  = createResponseCodedText(TestData)\n",
    "cv_text_feature_responseCoding  = createResponseCodedText(CV_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert each row values such that they sum to 1  \n",
    "train_text_feature_responseCoding = (train_text_feature_responseCoding.T/train_text_feature_responseCoding.sum(axis=1)).T\n",
    "test_text_feature_responseCoding = (test_text_feature_responseCoding.T/test_text_feature_responseCoding.sum(axis=1)).T\n",
    "cv_text_feature_responseCoding = (cv_text_feature_responseCoding.T/cv_text_feature_responseCoding.sum(axis=1)).T   \n",
    "    \n",
    "    \n",
    "# matx = [[1, 2],\n",
    "#        [3, 4]]\n",
    "# matx.T = [[1, 3],\n",
    "#           [2, 4]]\n",
    "# matx.sum(axis = 1)  axis=0 corresponds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "# matx.sum(axix =1) = [[3, 7]]\n",
    "# (matx.T)/(matx.sum(axis=1)) = [[1/3, 3/7]\n",
    "#                               [2/3, 4/7]]\n",
    "\n",
    "# (matx.T)/(matx.sum(axis=1)).T = [[1/3, 2/3]\n",
    "#                                  [3/7, 4/7]]\n",
    "# sum of row elements = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of response coded text feature of Train Data = \"+str(train_text_feature_responseCoding.shape))\n",
    "print(\"Size of response coded text feature of Test Data = \"+str(test_text_feature_responseCoding.shape))\n",
    "print(\"Size of response coded text feature of CV Data = \"+str(cv_text_feature_responseCoding.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>TFIDF</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1,2), stop_words = \"english\", min_df = 3, max_features = 100000)\n",
    "\n",
    "train_text_feature_tfidf = tfidf_vectorizer.fit_transform(TrainData['Text'])\n",
    "train_text_feature_tfidf = StandardScaler(with_mean = False).fit_transform(train_text_feature_tfidf)\n",
    "\n",
    "test_text_feature_tfidf = tfidf_vectorizer.transform(TestData['Text'])\n",
    "test_text_feature_tfidf = StandardScaler(with_mean = False).fit_transform(test_text_feature_tfidf)\n",
    "\n",
    "cv_text_feature_tfidf = tfidf_vectorizer.transform(CV_Data['Text'])\n",
    "cv_text_feature_tfidf = StandardScaler(with_mean = False).fit_transform(cv_text_feature_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of TFIDF coded text feature of Train Data = \"+str(train_text_feature_tfidf.shape))\n",
    "print(\"Size of TFIDF coded text feature of Test Data = \"+str(test_text_feature_tfidf.shape))\n",
    "print(\"Size of TFIDF coded text feature of CV Data = \"+str(cv_text_feature_tfidf.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are word frequencies distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(min_df = 3)\n",
    "TrainWordOccurrences = count_vec.fit_transform(TrainData['Text'])\n",
    "# getting all the feature names (words)\n",
    "TrainFeatures= count_vec.get_feature_names()\n",
    "\n",
    "# TrainWordOccurrences.sum(axis=0).A1 will sum every column and returns (1*number of features) vector\n",
    "wordOccurrenceDictionary = dict(zip(TrainFeatures,TrainWordOccurrences.sum(axis=0).A1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedWordOccurrenceDict = dict(sorted(wordOccurrenceDictionary.items(), key=lambda x: x[1] , reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedWordOccurrenceDictList = np.array(list(sortedWordOccurrenceDict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoOfWord_Occurrence = Counter(sortedWordOccurrenceDictList) \n",
    "#counter simply takes a list then it prints a dictionary. In that dictionary, keys are the number of elements and its values \n",
    "#are the number of times that element is occurring in that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NoOfWord_Occurrence)\n",
    "#this will print a dictionary where keys are number of word and its values are the number of times that word in appearing in\n",
    "#train data. For eg, in below dictionary 3 words are occurring 5316 times, 4 words are occurring 3385 times and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the text feature useful in predicitng y_i?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)]\n",
    "alpha.append(15)\n",
    "alpha.append(20)\n",
    "alpha.append(40)\n",
    "\n",
    "cv_log_loss = []\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_text_feature_tfidf, TrainDataLabels)\n",
    "    calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "    calib_clf.fit(train_text_feature_tfidf, TrainDataLabels)\n",
    "    predicted_y = calib_clf.predict_proba(cv_text_feature_tfidf)\n",
    "    cv_log_loss.append(log_loss(CV_Data_Labels, predicted_y, labels=clf.classes_))\n",
    "    print(\"For alpha value of \"+str(i)+\" CV log loss = \"+str(log_loss(CV_Data_Labels, predicted_y, labels=clf.classes_)))\n",
    "\n",
    "plt.figure(figsize = (12, 7))\n",
    "plt.plot(alpha, cv_log_loss)\n",
    "for xy in zip(alpha, np.round(cv_log_loss, 4)):\n",
    "    plt.annotate(xy, xy)\n",
    "    \n",
    "plt.title(\"Alpha vs Log-loss\", fontsize = 20)\n",
    "plt.xlabel(\"Alpha\", fontsize = 20)\n",
    "plt.ylabel(\"Log-Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alpha[np.argmin(cv_log_loss)]\n",
    "clf = SGDClassifier(alpha=best_alpha, penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_text_feature_tfidf, TrainDataLabels)\n",
    "calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "calib_clf.fit(train_text_feature_tfidf, TrainDataLabels)\n",
    "\n",
    "predict_train = calib_clf.predict_proba(train_text_feature_tfidf)\n",
    "print('For values of best alpha = ', best_alpha, \"the train log loss =:\",log_loss(TrainDataLabels, predict_train, labels=clf.classes_))\n",
    "\n",
    "predict_cv = calib_clf.predict_proba(cv_text_feature_tfidf)\n",
    "print('For values of best alpha = ', best_alpha, \"the CV log loss =:\",log_loss(CV_Data_Labels, predict_cv, labels=clf.classes_))\n",
    "\n",
    "predict_test = calib_clf.predict_proba(test_text_feature_tfidf)\n",
    "print('For values of best alpha = ', best_alpha, \"the test log loss =:\",log_loss(TestLabels, predict_test, labels=clf.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the text feature stable across train, test and CV datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkingCommonFeatures(df):\n",
    "    count_vect = CountVectorizer(min_df = 3)\n",
    "    BoWMatrx = count_vect.fit_transform(df[\"Text\"])\n",
    "    BoWMatrxFeatures = count_vect.get_feature_names()\n",
    "    BoWMatrxFeaturesUnique = len(set(BoWMatrxFeatures))\n",
    "    commonFeatures = len(set(TrainFeatures) & set(BoWMatrxFeatures))\n",
    "    return BoWMatrxFeaturesUnique, commonFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(min_df = 3)\n",
    "BoWMatrx = count_vect.fit_transform(TestData[\"Text\"])\n",
    "BoWMatrxFeatures = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len1, len2 = checkingCommonFeatures(TestData)\n",
    "print(\"Percentage of common features in train and test data = \"+str(np.round((len2/len1)*100, 4))+\"%\")\n",
    "#it prints: Out of total features in test data, how many features are also present in train data\n",
    "len3, len4 = checkingCommonFeatures(CV_Data)\n",
    "print(\"Percentage of common features in train and CV data = \"+str(np.round((len4/len3)*100, 4))+\"%\")\n",
    "#it prints: Out of total features in CV data, how many features are also present in train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Machine Learning Models</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the given indices, we will print the name of the features\n",
    "# and we will check whether the feature present in the test point text or not\n",
    "def getImportantFeatures(indices, gene, variation, text, noOfFeatures):\n",
    "    gene_vect = CountVectorizer()\n",
    "    var_vect = CountVectorizer()\n",
    "    text_vect = TfidfVectorizer(ngram_range = (1,2), stop_words = \"english\", min_df = 3, max_features = 100000)\n",
    "    \n",
    "    geneVect = gene_vect.fit(TrainData['Gene'])\n",
    "    varVect  = var_vect.fit(TrainData['Variation'])\n",
    "    textVect = text_vect.fit(TrainData['Text'])\n",
    "    \n",
    "    gene_features = gene_vect.get_feature_names()\n",
    "    variation_features = varVect.get_feature_names()\n",
    "    text_features = text_vect.get_feature_names()\n",
    "    \n",
    "    gene_feat_len = len(gene_features)\n",
    "    var_feat_len = len(variation_features)\n",
    "    \n",
    "    First = [x1 for x1 in range(0, 491, 10)]\n",
    "    del First[1]\n",
    "    Second = [x2 for x2 in range(1, 492, 10)]\n",
    "    del Second[1]\n",
    "    Third = [x3 for x3 in range(2, 493, 10)]\n",
    "    del Third[1]\n",
    "    \n",
    "    word_present = 0\n",
    "    for i, v in enumerate(indices):\n",
    "        if v < gene_feat_len:\n",
    "            word = gene_features[v]\n",
    "            \n",
    "            if word == gene:\n",
    "                word_present += 1\n",
    "                \n",
    "                if i in First:\n",
    "                    print(\"{}st Gene feature [{}] is present in query point\".format(i+1, word))\n",
    "                \n",
    "                elif i in Second:\n",
    "                    print(\"{}nd Gene feature [{}] is present in query point\".format(i+1, word))\n",
    "                \n",
    "                elif i in Third:\n",
    "                    print(\"{}rd Gene feature [{}] is present in query point\".format(i+1, word))\n",
    "                    \n",
    "                else:\n",
    "                    print(\"{}th Gen feature [{}] is present in query point\".format(i+1, word))\n",
    "                    \n",
    "        elif v < gene_feat_len + var_feat_len:\n",
    "            word = variation_features[v - gene_feat_len]\n",
    "            \n",
    "            if word == variation:\n",
    "                word_present += 1\n",
    "                if i in First:\n",
    "                    print(\"{}st Variation feature [{}] is present in query point\".format(i+1, word))\n",
    "                \n",
    "                elif i in Second:\n",
    "                    print(\"{}nd Variation feature [{}] is present in query point\".format(i+1, word))\n",
    "                \n",
    "                elif i in Third:\n",
    "                    print(\"{}rd Variation feature [{}] is present in query point\".format(i+1, word))\n",
    "                    \n",
    "                else:\n",
    "                    print(\"{}th Variation feature [{}] is present in query point\".format(i+1, word))\n",
    "        else:\n",
    "            word = text_features[v - (gene_feat_len + var_feat_len)]\n",
    "            \n",
    "            if word in text.split():\n",
    "                word_present += 1\n",
    "                \n",
    "                if i in First:\n",
    "                    print(\"{}st Text feature [{}] is present in query point\".format(i+1, word))\n",
    "                \n",
    "                elif i in Second:\n",
    "                    print(\"{}nd Text feature [{}] is present in query point\".format(i+1, word))\n",
    "                \n",
    "                elif i in Third:\n",
    "                    print(\"{}rd Text feature [{}] is present in query point\".format(i+1, word))\n",
    "                    \n",
    "                else:\n",
    "                    print(\"{}th Text feature [{}] is present in query point\".format(i+1, word))\n",
    "                    \n",
    "    print(\"-\"*63)                \n",
    "    print(\"Out of the top \"+str(noOfFeatures)+\" features \"+str(word_present)+\" are present in query point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(columns = [\"Model\", \"Train Log-loss\", \"CV Log-loss\", \"Test Log-loss\", \"Mis-Classified CV\", \"Mis-Classified Test\", \"Remarks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Stacking three types of classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [[1, 2], \n",
    "#      [3, 4]]\n",
    "# b = [[4, 5], \n",
    "#      [6, 7]]\n",
    "# hstack(a, b) = [[1, 2, 4, 5],\n",
    "#                [ 3, 4, 6, 7]]\n",
    "\n",
    "train_gene_var_oneHotEncode = hstack((train_gene_feature_onehotCoding, train_variation_feature_onehotCoding))\n",
    "cv_gene_var_oneHotEncode = hstack((cv_gene_feature_onehotCoding, cv_variation_feature_onehotCoding))\n",
    "test_gene_var_oneHotEncode = hstack((test_gene_feature_onehotCoding, test_variation_feature_onehotCoding))\n",
    "\n",
    "Train_X = hstack((train_gene_var_oneHotEncode, train_text_feature_tfidf))\n",
    "Train_X = Train_X.tocsr()\n",
    "Train_Y = np.array(list(TrainDataLabels))\n",
    "\n",
    "CV_X = hstack((cv_gene_var_oneHotEncode, cv_text_feature_tfidf))\n",
    "CV_X = CV_X.tocsr()\n",
    "CV_Y = np.array(list(CV_Data_Labels))\n",
    "\n",
    "Test_X = hstack((test_gene_var_oneHotEncode, test_text_feature_tfidf))\n",
    "Test_X = Test_X.tocsr()\n",
    "Test_Y = np.array(list(TestLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of One hot encoded Gene and Variation, TFIDF text stacked vector for Train Data = \"+str(Train_X.shape))\n",
    "print(\"Shape of One hot encoded Gene and Variation, TFIDF text stacked vector for CV Data = \"+str(CV_X.shape))\n",
    "print(\"Shape of One hot encoded Gene and Variation, TFIDF text stacked vector for Test Data = \"+str(Test_X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gene_var_responseCoded = np.hstack((train_gene_feature_responseCoding, train_variation_feature_responseCoding))\n",
    "cv_gene_var_responseCoded = np.hstack((cv_gene_feature_responseCoding, cv_variation_feature_responseCoding))\n",
    "test_gene_var_responseCoded = np.hstack((test_gene_feature_responseCoding, test_variation_feature_responseCoding))\n",
    "\n",
    "Train_X_ResponseCoded = np.hstack((train_gene_var_responseCoded, train_text_feature_responseCoding))\n",
    "CV_X_ResponseCoded = np.hstack((cv_gene_var_responseCoded, cv_text_feature_responseCoding))\n",
    "Test_X_ResponseCoded = np.hstack((test_gene_var_responseCoded, test_text_feature_responseCoding))\n",
    "\n",
    "# Train_X_ResponseCoded = StandardScaler().fit_transform(Train_X_ResponseCoded)\n",
    "# CV_X_ResponseCoded = StandardScaler().fit_transform(CV_X_ResponseCoded)\n",
    "# Test_X_ResponseCoded = StandardScaler().fit_transform(Test_X_ResponseCoded)\n",
    "\n",
    "#Here, Response coded vectors are not standardized because all the column are just probabilities between 0 and 1 and therefore,\n",
    "#all are in same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Response Coded Gene and Variation, TFIDF text stacked vector for Train Data = \"+str(Train_X_ResponseCoded.shape))\n",
    "print(\"Shape of Response Coded Gene and Variation, TFIDF text stacked vector for CV Data = \"+str(CV_X_ResponseCoded.shape))\n",
    "print(\"Shape of Response Coded Gene and Variation, TFIDF text stacked vector for Test Data = \"+str(Test_X_ResponseCoded.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Line Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10**x for x in range(-5, 5)]\n",
    "\n",
    "cv_log_loss = []\n",
    "for i in alpha:\n",
    "    clf = MultinomialNB(alpha=i)\n",
    "    clf.fit(Train_X, Train_Y)\n",
    "    calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "    \n",
    "    #The CalibratedClassifierCV class is used to calibrate a classifier. CalibratedClassifierCV uses a cross-validation\n",
    "    #approach to ensure unbiased data is always used to fit the calibrator. The data is split into k (train_set, test_set) \n",
    "    #couples (as determined by cv ).\n",
    "    \n",
    "    calib_clf.fit(Train_X, Train_Y)\n",
    "    predicted_y = calib_clf.predict_proba(CV_X)\n",
    "    cv_log_loss.append(log_loss(CV_Y, predicted_y, labels=clf.classes_))\n",
    "    print(\"For alpha value of \"+str(i)+\" CV log loss = \"+str(log_loss(CV_Y, predicted_y, labels=clf.classes_)))\n",
    "\n",
    "plt.figure(figsize = (12, 7))\n",
    "plt.plot(alpha, cv_log_loss)\n",
    "for xy in zip(alpha, np.round(cv_log_loss, 4)):\n",
    "    plt.annotate(xy, xy)\n",
    "    \n",
    "plt.title(\"Alpha vs Log-loss\", fontsize = 20)\n",
    "plt.xlabel(\"Alpha\", fontsize = 20)\n",
    "plt.ylabel(\"Log-Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing with best hyper-parameter</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alpha[np.argmin(cv_log_loss)]\n",
    "clf = MultinomialNB(alpha=best_alpha)\n",
    "clf.fit(Train_X, Train_Y)\n",
    "calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "calib_clf.fit(Train_X, Train_Y)\n",
    "\n",
    "predict_train = calib_clf.predict_proba(Train_X)\n",
    "print('For values of best alpha = ', best_alpha, \"the train log loss =:\",log_loss(Train_Y, predict_train, labels=clf.classes_))\n",
    "\n",
    "predict_cv = calib_clf.predict_proba(CV_X)\n",
    "print('For values of best alpha = ', best_alpha, \"the CV log loss =:\",log_loss(CV_Y, predict_cv, labels=clf.classes_))\n",
    "\n",
    "predict_test = calib_clf.predict_proba(Test_X)\n",
    "print('For values of best alpha = ', best_alpha, \"the test log loss =:\",log_loss(Test_Y, predict_test, labels=clf.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of mis-classified for CV points = \"+str(np.round((np.count_nonzero(calib_clf.predict(CV_X) - CV_Y)/CV_X.shape[0]*100), 2))+\"%\")\n",
    "print(\"Percentage of mis-classified for Test points = \"+str(np.round((np.count_nonzero(calib_clf.predict(Test_X) - Test_Y)/Test_X.shape[0]*100), 2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.append(pd.DataFrame([[\"Naive Bayes\", 0.9031, 1.3471, 1.2976, \"42.75%\", \"39.46%\", \"GoodFit\"]], columns = [\"Model\", \"Train Log-loss\", \"CV Log-loss\", \"Test Log-loss\", \"Mis-Classified CV\", \"Mis-Classified Test\", \"Remarks\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusionMatrix(Test_Y, calib_clf.predict(Test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking first 100 important features for correctly classified test point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testDataPoint = 5\n",
    "no_feature = 100\n",
    "predicted_cls = calib_clf.predict(Test_X[testDataPoint])\n",
    "TrueLabel = Test_Y[testDataPoint]\n",
    "predicted_prob = np.round(calib_clf.predict_proba(Test_X[testDataPoint]), 4)\n",
    "print(\"Predicted Class label for test point = \"+str(predicted_cls[0]))\n",
    "print(\"Predicted Probabilities for test point = \"+str(predicted_prob))\n",
    "print(\"True class label for test point = \"+str(TrueLabel))\n",
    "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
    "print(\"-\"*63)\n",
    "getImportantFeatures(indices[0], TestData.iloc[testDataPoint][\"Gene\"], TestData.iloc[testDataPoint][\"Variation\"], TestData.iloc[testDataPoint][\"Text\"], no_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking first 100 important features for incorrectly classified test point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataPoint = 10\n",
    "no_feature = 100\n",
    "predicted_cls = calib_clf.predict(Test_X[testDataPoint])\n",
    "TrueLabel = Test_Y[testDataPoint]\n",
    "predicted_prob = np.round(calib_clf.predict_proba(Test_X[testDataPoint]), 4)\n",
    "print(\"Predicted Class label for test point = \"+str(predicted_cls[0]))\n",
    "print(\"Predicted Probabilities for test point \", end='')\n",
    "print(predicted_prob[0])\n",
    "print(\"True class label for test point = \"+str(TrueLabel))\n",
    "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
    "print(\"-\"*63)\n",
    "getImportantFeatures(indices[0], TestData.iloc[testDataPoint][\"Gene\"], TestData.iloc[testDataPoint][\"Variation\"], TestData.iloc[testDataPoint][\"Text\"], no_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = [3, 5, 8, 13, 23, 35, 51, 71, 95, 121, 151, 181, 221]\n",
    "\n",
    "cv_log_loss = []\n",
    "for i in neighbors:\n",
    "    clf = KNeighborsClassifier(n_neighbors = i, n_jobs = -1)\n",
    "    clf.fit(Train_X_ResponseCoded, Train_Y)\n",
    "    calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "    calib_clf.fit(Train_X_ResponseCoded, Train_Y)\n",
    "    predicted_y = calib_clf.predict_proba(CV_X_ResponseCoded)\n",
    "    cv_log_loss.append(log_loss(CV_Y, predicted_y, labels=clf.classes_))\n",
    "    print(\"For Neighbor value of \"+str(i)+\" CV log loss = \"+str(log_loss(CV_Y, predicted_y, labels=clf.classes_)))\n",
    "\n",
    "plt.figure(figsize = (12, 7))\n",
    "plt.plot(neighbors, cv_log_loss)\n",
    "for xy in zip(neighbors, np.round(cv_log_loss, 4)):\n",
    "    plt.annotate(xy, xy)\n",
    "    \n",
    "plt.title(\"Neighbors vs Log-loss\", fontsize = 20)\n",
    "plt.xlabel(\"Neighbors\", fontsize = 20)\n",
    "plt.ylabel(\"Log-Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing with best hyper-parameter</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_neighbors = neighbors[np.argmin(cv_log_loss)]\n",
    "clf = KNeighborsClassifier(n_neighbors = best_neighbors, n_jobs = -1)\n",
    "clf.fit(Train_X_ResponseCoded, Train_Y)\n",
    "calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "calib_clf.fit(Train_X_ResponseCoded, Train_Y)\n",
    "\n",
    "predict_train = calib_clf.predict_proba(Train_X_ResponseCoded)\n",
    "print('For values of best neighbors = ', best_neighbors, \"the train log loss =:\",log_loss(Train_Y, predict_train, labels=clf.classes_))\n",
    "\n",
    "predict_cv = calib_clf.predict_proba(CV_X_ResponseCoded)\n",
    "print('For values of best neighbors = ', best_neighbors, \"the CV log loss =:\",log_loss(CV_Y, predict_cv, labels=clf.classes_))\n",
    "\n",
    "predict_test = calib_clf.predict_proba(Test_X_ResponseCoded)\n",
    "print('For values of best neighbors = ', best_neighbors, \"the test log loss =:\",log_loss(Test_Y, predict_test, labels=clf.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of mis-classified for CV points = \"+str(np.round((np.count_nonzero(calib_clf.predict(CV_X_ResponseCoded) - CV_Y)/CV_X_ResponseCoded.shape[0]*100), 2))+\"%\")\n",
    "print(\"Percentage of mis-classified for Test points = \"+str(np.round((np.count_nonzero(calib_clf.predict(Test_X_ResponseCoded) - Test_Y)/Test_X_ResponseCoded.shape[0]*100), 2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.append(pd.DataFrame([[\"KNN\", 0.0952, 1.4864, 1.529, \"45.39%\", \"44.88%\", \"OverFit\"]], columns = [\"Model\", \"Train Log-loss\", \"CV Log-loss\", \"Test Log-loss\", \"Mis-Classified CV\", \"Mis-Classified Test\", \"Remarks\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_confusionMatrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dd0c1e4a9f9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint_confusionMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTest_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcalib_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTest_X_ResponseCoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'print_confusionMatrix' is not defined"
     ]
    }
   ],
   "source": [
    "print_confusionMatrix(Test_Y, calib_clf.predict(Test_X_ResponseCoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking Nearest Neighbors for correctly classified point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataPoint = 35\n",
    "predicted_cls = calib_clf.predict(Test_X_ResponseCoded[testDataPoint].reshape(1, -1)) #it will convert vector into 2D.\n",
    "TrueLabel = Test_Y[testDataPoint]\n",
    "predicted_prob = np.round(calib_clf.predict_proba(Test_X_ResponseCoded[testDataPoint].reshape(1, -1)), 2)\n",
    "print(\"Predicted Class label for test point = \"+str(predicted_cls[0]))\n",
    "print(\"Predicted Probabilities for test point \", end='')\n",
    "print(predicted_prob[0])\n",
    "print(\"True class label for test point = \"+str(TrueLabel)+\"\\n\")\n",
    "nearest_neighbor_points = clf.kneighbors(Test_X_ResponseCoded[testDataPoint].reshape(1, -1), n_neighbors = best_neighbors, return_distance = False)\n",
    "print(\"Labels of nearest neighbors to test points = \"+str(Train_Y[nearest_neighbor_points][0]))\n",
    "print(\"Class Label: Number of neighboring points = \"+str(Counter(Train_Y[nearest_neighbor_points][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking Nearest Neighbors for incorrectly classified point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataPoint = 100\n",
    "predicted_cls = calib_clf.predict(Test_X_ResponseCoded[testDataPoint].reshape(1, -1)) #it will convert vector into 2D.\n",
    "TrueLabel = Test_Y[testDataPoint]\n",
    "predicted_prob = np.round(calib_clf.predict_proba(Test_X_ResponseCoded[testDataPoint].reshape(1, -1)), 2)\n",
    "print(\"Predicted Class label for test point = \"+str(predicted_cls[0]))\n",
    "print(\"Predicted Probabilities for test point \", end='')\n",
    "print(predicted_prob[0])\n",
    "print(\"True class label for test point = \"+str(TrueLabel)+\"\\n\")\n",
    "nearest_neighbor_points = clf.kneighbors(Test_X_ResponseCoded[testDataPoint].reshape(1, -1), n_neighbors = best_neighbors, return_distance = False)\n",
    "print(\"The best value of nearest neighbors is \"+str(best_neighbors)+\" and class labels of those nearest neighbors to test points = \"+str(Train_Y[nearest_neighbor_points][0])+\"\\n\")\n",
    "print(\"Class Label: Number of neighboring points = \"+str(Counter(Train_Y[nearest_neighbor_points][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with class balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "alpha = [10**x for x in range(-5, 5)]\n",
    "\n",
    "cv_log_loss = []\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(loss = \"log\", alpha = i, class_weight = \"balanced\")\n",
    "    clf.fit(Train_X, Train_Y)\n",
    "    calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "    calib_clf.fit(Train_X, Train_Y)\n",
    "    predicted_y = calib_clf.predict_proba(CV_X)\n",
    "    cv_log_loss.append(log_loss(CV_Y, predicted_y, labels=clf.classes_))\n",
    "    print(\"For alpha value of \"+str(i)+\" CV log loss = \"+str(log_loss(CV_Y, predicted_y, labels=clf.classes_)))\n",
    "\n",
    "plt.figure(figsize = (12, 7))\n",
    "plt.plot(alpha, cv_log_loss)\n",
    "for xy in zip(alpha, np.round(cv_log_loss, 4)):\n",
    "    plt.annotate(xy, xy)\n",
    "    \n",
    "plt.title(\"Alpha vs Log-loss\", fontsize = 20)\n",
    "plt.xlabel(\"Alpha\", fontsize = 20)\n",
    "plt.ylabel(\"Log-Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing with best hyper-parameter</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alpha[np.argmin(cv_log_loss)]\n",
    "clf = SGDClassifier(loss = \"log\", alpha = best_alpha, class_weight = \"balanced\")\n",
    "clf.fit(Train_X, Train_Y)\n",
    "calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "calib_clf.fit(Train_X, Train_Y)\n",
    "\n",
    "predict_train = calib_clf.predict_proba(Train_X)\n",
    "print('For values of best alpha = ', best_alpha, \"the train log loss =:\",log_loss(Train_Y, predict_train, labels=clf.classes_))\n",
    "\n",
    "predict_cv = calib_clf.predict_proba(CV_X)\n",
    "print('For values of best alpha = ', best_alpha, \"the CV log loss =:\",log_loss(CV_Y, predict_cv, labels=clf.classes_))\n",
    "\n",
    "predict_test = calib_clf.predict_proba(Test_X)\n",
    "print('For values of best alpha = ', best_alpha, \"the test log loss =:\",log_loss(Test_Y, predict_test, labels=clf.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of mis-classified for CV points = \"+str(np.round((np.count_nonzero(calib_clf.predict(CV_X) - CV_Y)/CV_X.shape[0]*100), 2))+\"%\")\n",
    "print(\"Percentage of mis-classified for Test points = \"+str(np.round((np.count_nonzero(calib_clf.predict(Test_X) - Test_Y)/Test_X.shape[0]*100), 2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.append(pd.DataFrame([[\"Logistic Regresion(Balanced)\", 0.6738, 1.1314, 1.101, \"40.49%\", \"36.9%\", \"Good Fit\"]], columns = [\"Model\", \"Train Log-loss\", \"CV Log-loss\", \"Test Log-loss\", \"Mis-Classified CV\", \"Mis-Classified Test\", \"Remarks\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusionMatrix(Test_Y, calib_clf.predict(Test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking first 500 important features for correctly classified test point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testDataPoint = 5\n",
    "no_feature = 500\n",
    "predicted_cls = calib_clf.predict(Test_X[testDataPoint])\n",
    "TrueLabel = Test_Y[testDataPoint]\n",
    "predicted_prob = np.round(calib_clf.predict_proba(Test_X[testDataPoint]), 3)\n",
    "print(\"Predicted Class label for test point = \"+str(predicted_cls[0]))\n",
    "print(\"Predicted Probabilities for test point = \"+str(predicted_prob))\n",
    "print(\"True class label for test point = \"+str(TrueLabel))\n",
    "indices = np.argsort(-abs(clf.coef_))[predicted_cls-1][:,:no_feature]\n",
    "print(\"-\"*63)\n",
    "getImportantFeatures(indices[0], TestData.iloc[testDataPoint][\"Gene\"], TestData.iloc[testDataPoint][\"Variation\"], TestData.iloc[testDataPoint][\"Text\"], no_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking first 500 important features for incorrectly classified test point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testDataPoint = 10\n",
    "no_feature = 500\n",
    "predicted_cls = calib_clf.predict(Test_X[testDataPoint])\n",
    "TrueLabel = Test_Y[testDataPoint]\n",
    "predicted_prob = np.round(calib_clf.predict_proba(Test_X[testDataPoint]), 3)\n",
    "print(\"Predicted Class label for test point = \"+str(predicted_cls[0]))\n",
    "print(\"Predicted Probabilities for test point = \"+str(predicted_prob))\n",
    "print(\"True class label for test point = \"+str(TrueLabel))\n",
    "indices = np.argsort(-abs(clf.coef_))[predicted_cls-1][:,:no_feature]\n",
    "print(\"-\"*63)\n",
    "getImportantFeatures(indices[0], TestData.iloc[testDataPoint][\"Gene\"], TestData.iloc[testDataPoint][\"Variation\"], TestData.iloc[testDataPoint][\"Text\"], no_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression without class balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "alpha = [10**x for x in range(-5, 5)]\n",
    "\n",
    "cv_log_loss = []\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(loss = \"log\", alpha = i)\n",
    "    clf.fit(Train_X, Train_Y)\n",
    "    calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "    calib_clf.fit(Train_X, Train_Y)\n",
    "    predicted_y = calib_clf.predict_proba(CV_X)\n",
    "    cv_log_loss.append(log_loss(CV_Y, predicted_y, labels=clf.classes_))\n",
    "    print(\"For alpha value of \"+str(i)+\" CV log loss = \"+str(log_loss(CV_Y, predicted_y, labels=clf.classes_)))\n",
    "\n",
    "plt.figure(figsize = (12, 7))\n",
    "plt.plot(alpha, cv_log_loss)\n",
    "for xy in zip(alpha, np.round(cv_log_loss, 4)):\n",
    "    plt.annotate(xy, xy)\n",
    "    \n",
    "plt.title(\"Alpha vs Log-loss\", fontsize = 20)\n",
    "plt.xlabel(\"Alpha\", fontsize = 20)\n",
    "plt.ylabel(\"Log-Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing with best hyper-parameter</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alpha[np.argmin(cv_log_loss)]\n",
    "clf = SGDClassifier(loss = \"log\", alpha = best_alpha)\n",
    "clf.fit(Train_X, Train_Y)\n",
    "calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "calib_clf.fit(Train_X, Train_Y)\n",
    "\n",
    "predict_train = calib_clf.predict_proba(Train_X)\n",
    "print('For values of best alpha = ', best_alpha, \"the train log loss =:\",log_loss(Train_Y, predict_train, labels=clf.classes_))\n",
    "\n",
    "predict_cv = calib_clf.predict_proba(CV_X)\n",
    "print('For values of best alpha = ', best_alpha, \"the CV log loss =:\",log_loss(CV_Y, predict_cv, labels=clf.classes_))\n",
    "\n",
    "predict_test = calib_clf.predict_proba(Test_X)\n",
    "print('For values of best alpha = ', best_alpha, \"the test log loss =:\",log_loss(Test_Y, predict_test, labels=clf.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of mis-classified for CV points = \"+str(np.round((np.count_nonzero(calib_clf.predict(CV_X) - CV_Y)/CV_X.shape[0]*100), 2))+\"%\")\n",
    "print(\"Percentage of mis-classified for Test points = \"+str(np.round((np.count_nonzero(calib_clf.predict(Test_X) - Test_Y)/Test_X.shape[0]*100), 2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.append(pd.DataFrame([[\"Logistic Regresion(Imbalanced)\", 0.6640, 1.1335, 1.0956, \"39.36%\", \"37.5%\", \"Good Fit\"]], columns = [\"Model\", \"Train Log-loss\", \"CV Log-loss\", \"Test Log-loss\", \"Mis-Classified CV\", \"Mis-Classified Test\", \"Remarks\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusionMatrix(Test_Y, calib_clf.predict(Test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking first 500 important features for correctly classified test point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataPoint = 5\n",
    "no_feature = 500\n",
    "predicted_cls = calib_clf.predict(Test_X[testDataPoint])\n",
    "TrueLabel = Test_Y[testDataPoint]\n",
    "predicted_prob = np.round(calib_clf.predict_proba(Test_X[testDataPoint]), 3)\n",
    "print(\"Predicted Class label for test point = \"+str(predicted_cls[0]))\n",
    "print(\"Predicted Probabilities for test point = \"+str(predicted_prob))\n",
    "print(\"True class label for test point = \"+str(TrueLabel))\n",
    "indices = np.argsort(-abs(clf.coef_))[predicted_cls-1][:,:no_feature]\n",
    "print(\"-\"*63)\n",
    "getImportantFeatures(indices[0], TestData.iloc[testDataPoint][\"Gene\"], TestData.iloc[testDataPoint][\"Variation\"], TestData.iloc[testDataPoint][\"Text\"], no_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking first 500 important features for incorrectly classified test point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataPoint = 25\n",
    "no_feature = 500\n",
    "predicted_cls = calib_clf.predict(Test_X[testDataPoint])\n",
    "TrueLabel = Test_Y[testDataPoint]\n",
    "predicted_prob = np.round(calib_clf.predict_proba(Test_X[testDataPoint]), 4)\n",
    "print(\"Predicted Class label for test point = \"+str(predicted_cls[0]))\n",
    "print(\"Predicted Probabilities for test point = \"+str(predicted_prob))\n",
    "print(\"True class label for test point = \"+str(TrueLabel))\n",
    "indices = np.argsort(-abs(clf.coef_))[predicted_cls-1][:,:no_feature]\n",
    "print(\"-\"*63)\n",
    "getImportantFeatures(indices[0], TestData.iloc[testDataPoint][\"Gene\"], TestData.iloc[testDataPoint][\"Variation\"], TestData.iloc[testDataPoint][\"Text\"], no_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10**x for x in range(-5, 5)]\n",
    "\n",
    "cv_log_loss = []\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(loss = \"hinge\", alpha = i, class_weight = \"balanced\")\n",
    "    clf.fit(Train_X, Train_Y)\n",
    "    calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "    calib_clf.fit(Train_X, Train_Y)\n",
    "    predicted_y = calib_clf.predict_proba(CV_X)\n",
    "    cv_log_loss.append(log_loss(CV_Y, predicted_y, labels=clf.classes_))\n",
    "    print(\"For alpha value of \"+str(i)+\" CV log loss = \"+str(log_loss(CV_Y, predicted_y, labels=clf.classes_)))\n",
    "\n",
    "plt.figure(figsize = (12, 7))\n",
    "plt.plot(alpha, cv_log_loss)\n",
    "for xy in zip(alpha, np.round(cv_log_loss, 4)):\n",
    "    plt.annotate(xy, xy)\n",
    "    \n",
    "plt.title(\"Alpha vs Log-loss\", fontsize = 20)\n",
    "plt.xlabel(\"Alpha\", fontsize = 20)\n",
    "plt.ylabel(\"Log-Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing with best best Hyper-Parameter</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alpha[np.argmin(cv_log_loss)]\n",
    "clf = SGDClassifier(loss = \"hinge\", alpha = best_alpha, class_weight = \"balanced\")\n",
    "clf.fit(Train_X, Train_Y)\n",
    "calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "calib_clf.fit(Train_X, Train_Y)\n",
    "\n",
    "predict_train = calib_clf.predict_proba(Train_X)\n",
    "print('For values of best alpha = ', best_alpha, \"the train log loss =:\",log_loss(Train_Y, predict_train, labels=clf.classes_))\n",
    "\n",
    "predict_cv = calib_clf.predict_proba(CV_X)\n",
    "print('For values of best alpha = ', best_alpha, \"the CV log loss =:\",log_loss(CV_Y, predict_cv, labels=clf.classes_))\n",
    "\n",
    "predict_test = calib_clf.predict_proba(Test_X)\n",
    "print('For values of best alpha = ', best_alpha, \"the test log loss =:\",log_loss(Test_Y, predict_test, labels=clf.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of mis-classified for CV points = \"+str(np.round((np.count_nonzero(calib_clf.predict(CV_X) - CV_Y)/CV_X.shape[0]*100), 2))+\"%\")\n",
    "print(\"Percentage of mis-classified for Test points = \"+str(np.round((np.count_nonzero(calib_clf.predict(Test_X) - Test_Y)/Test_X.shape[0]*100), 2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.append(pd.DataFrame([[\"Linear SVM(Balanced)\", 0.8022, 1.2399, 1.2217, \"39.36%\", \"38.7%\", \"Good Fit\"]], columns = [\"Model\", \"Train Log-loss\", \"CV Log-loss\", \"Test Log-loss\", \"Mis-Classified CV\", \"Mis-Classified Test\", \"Remarks\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusionMatrix(Test_Y, calib_clf.predict(Test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking first 500 important features for correctly classified test point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataPoint = 5\n",
    "no_feature = 500\n",
    "predicted_cls = calib_clf.predict(Test_X[testDataPoint])\n",
    "TrueLabel = Test_Y[testDataPoint]\n",
    "predicted_prob = np.round(calib_clf.predict_proba(Test_X[testDataPoint]), 4)\n",
    "print(\"Predicted Class label for test point = \"+str(predicted_cls[0]))\n",
    "print(\"Predicted Probabilities for test point = \"+str(predicted_prob))\n",
    "print(\"True class label for test point = \"+str(TrueLabel))\n",
    "indices = np.argsort(-abs(clf.coef_))[predicted_cls-1][:,:no_feature]\n",
    "print(\"-\"*63)\n",
    "getImportantFeatures(indices[0], TestData.iloc[testDataPoint][\"Gene\"], TestData.iloc[testDataPoint][\"Variation\"], TestData.iloc[testDataPoint][\"Text\"], no_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking first 500 important features for incorrectly classified test point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataPoint = 25\n",
    "no_feature = 500\n",
    "predicted_cls = calib_clf.predict(Test_X[testDataPoint])\n",
    "TrueLabel = Test_Y[testDataPoint]\n",
    "predicted_prob = np.round(calib_clf.predict_proba(Test_X[testDataPoint]), 4)\n",
    "print(\"Predicted Class label for test point = \"+str(predicted_cls[0]))\n",
    "print(\"Predicted Probabilities for test point = \"+str(predicted_prob))\n",
    "print(\"True class label for test point = \"+str(TrueLabel))\n",
    "indices = np.argsort(-abs(clf.coef_))[predicted_cls-1][:,:no_feature]\n",
    "print(\"-\"*63)\n",
    "getImportantFeatures(indices[0], TestData.iloc[testDataPoint][\"Gene\"], TestData.iloc[testDataPoint][\"Variation\"], TestData.iloc[testDataPoint][\"Text\"], no_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learners = [100, 200, 500, 1000]\n",
    "max_depth_baseLearners = [7, 12]\n",
    "cv_log_loss = []\n",
    "\n",
    "for i in base_learners:\n",
    "    for j in max_depth_baseLearners:\n",
    "        clf = RandomForestClassifier(n_estimators = i, max_depth = j, n_jobs = -1)\n",
    "        clf.fit(Train_X, Train_Y)\n",
    "        calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "        calib_clf.fit(Train_X, Train_Y)\n",
    "        predicted_y = calib_clf.predict_proba(CV_X)\n",
    "        cv_log_loss.append(log_loss(CV_Y, predicted_y, labels=clf.classes_))\n",
    "        print(\"For Number of base learners \"+str(i)+\" and max depth of a tree \"+str(j)+\" CV log loss = \"+str(log_loss(CV_Y, predicted_y, labels=clf.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridLogLoss = []\n",
    "subLogLoss = []\n",
    "x = [x for x in range(1, 8, 2)]\n",
    "for i in range(8):\n",
    "    subLogLoss.append(np.round(cv_log_loss[i], 4))\n",
    "    if i in x:\n",
    "        gridLogLoss.append(subLogLoss)\n",
    "        subLogLoss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridLogLossFrame = pd.DataFrame(gridLogLoss, columns = max_depth_baseLearners)\n",
    "gridLogLossFrame[\"Base_Learners\"] = base_learners\n",
    "gridLogLossFrame.set_index(\"Base_Learners\", append = False, drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.title(\"Log loss for Maximum Depth VS No of Base Learners\", size = 15)\n",
    "ax = sns.heatmap(gridLogLossFrame, annot = True, cmap=\"PuOr\", linewidths = 0.5, fmt = \".5g\", annot_kws={\"size\": 20})\n",
    "ax.figure.axes[0].set_xlabel(\"Maximum Depth\", size = 15)\n",
    "ax.figure.axes[0].set_ylabel(\"No of Base Learners\", size = 15)\n",
    "ax.figure.axes[-1].set_xlabel(\"Log Loss\", size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing with best Hyper Parameter</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.argmin(cv_log_loss)\n",
    "best_estimator = base_learners[int(best/2)]\n",
    "best_depth = max_depth_baseLearners[int(best/4)]\n",
    "clf = RandomForestClassifier(n_estimators = best_estimator, max_depth = best_depth, n_jobs = -1)\n",
    "clf.fit(Train_X, Train_Y)\n",
    "calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "calib_clf.fit(Train_X, Train_Y)\n",
    "\n",
    "predict_train = calib_clf.predict_proba(Train_X)\n",
    "print(\"For Number of base learners \"+str(best_estimator)+\" and max depth of a tree \"+str(best_depth)+\" Train log loss = \"+str(log_loss(Train_Y, predict_train, labels=clf.classes_)))\n",
    "\n",
    "predict_cv = calib_clf.predict_proba(CV_X)\n",
    "print(\"For Number of base learners \"+str(best_estimator)+\" and max depth of a tree \"+str(best_depth)+\" CV log loss = \"+str(log_loss(CV_Y, predict_cv, labels=clf.classes_)))\n",
    "\n",
    "predict_test = calib_clf.predict_proba(Test_X)\n",
    "print(\"For Number of base learners \"+str(best_estimator)+\" and max depth of a tree \"+str(best_depth)+\" Test log loss = \"+str(log_loss(Test_Y, predict_test, labels=clf.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of mis-classified for CV points = \"+str(np.round((np.count_nonzero(calib_clf.predict(CV_X) - CV_Y)/CV_X.shape[0]*100), 2))+\"%\")\n",
    "print(\"Percentage of mis-classified for Test points = \"+str(np.round((np.count_nonzero(calib_clf.predict(Test_X) - Test_Y)/Test_X.shape[0]*100), 2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.append(pd.DataFrame([[\"Random Forest\", 0.6165, 1.0961, 1.0994, \"38.23%\", \"35.69%\", \"Best Fit\"]], columns = [\"Model\", \"Train Log-loss\", \"CV Log-loss\", \"Test Log-loss\", \"Mis-Classified CV\", \"Mis-Classified Test\", \"Remarks\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusionMatrix(Test_Y, calib_clf.predict(Test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking first 100 important features for correctly classified test point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testDataPoint = 5\n",
    "no_feature = 100\n",
    "predicted_cls = calib_clf.predict(Test_X[testDataPoint])\n",
    "TrueLabel = Test_Y[testDataPoint]\n",
    "predicted_prob = np.round(calib_clf.predict_proba(Test_X[testDataPoint]), 4)\n",
    "print(\"Predicted Class label for test point = \"+str(predicted_cls[0]))\n",
    "print(\"Predicted Probabilities for test point = \"+str(predicted_prob))\n",
    "print(\"True class label for test point = \"+str(TrueLabel))\n",
    "indices = np.argsort(-clf.feature_importances_)\n",
    "print(\"-\"*63)\n",
    "getImportantFeatures(indices[:no_feature], TestData.iloc[testDataPoint][\"Gene\"], TestData.iloc[testDataPoint][\"Variation\"], TestData.iloc[testDataPoint][\"Text\"], no_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking first 100 important features for incorrectly classified test point</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataPoint = 25\n",
    "no_feature = 100\n",
    "predicted_cls = calib_clf.predict(Test_X[testDataPoint])\n",
    "TrueLabel = Test_Y[testDataPoint]\n",
    "predicted_prob = np.round(calib_clf.predict_proba(Test_X[testDataPoint]), 4)\n",
    "print(\"Predicted Class label for test point = \"+str(predicted_cls[0]))\n",
    "print(\"Predicted Probabilities for test point = \"+str(predicted_prob))\n",
    "print(\"True class label for test point = \"+str(TrueLabel))\n",
    "indices = np.argsort(-clf.feature_importances_)\n",
    "print(\"-\"*63)\n",
    "getImportantFeatures(indices[:no_feature], TestData.iloc[testDataPoint][\"Gene\"], TestData.iloc[testDataPoint][\"Variation\"], TestData.iloc[testDataPoint][\"Text\"], no_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking 4 best performing Models with their best hyper-parameters: Naive Bayes, Logistic Regression, Linear SVM, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_NB = MultinomialNB(alpha=10**-5)\n",
    "clf_NB.fit(Train_X, Train_Y)\n",
    "calib_clf_NB = CalibratedClassifierCV(clf_NB, method = \"sigmoid\")\n",
    "calib_clf_NB.fit(Train_X, Train_Y)\n",
    "print(\"Naive Bayes CV Log Loss: \"+str(np.round(log_loss(CV_Y, calib_clf_NB.predict_proba(CV_X), labels=clf_NB.classes_), 4)))\n",
    "\n",
    "clf_LR = SGDClassifier(loss = \"log\", alpha = 10, class_weight = \"balanced\")\n",
    "clf_LR.fit(Train_X, Train_Y)\n",
    "calib_clf_LR = CalibratedClassifierCV(clf_LR, method = \"sigmoid\")\n",
    "calib_clf_LR.fit(Train_X, Train_Y)\n",
    "print(\"Logistic Regression CV Log Loss: \"+str(np.round(log_loss(CV_Y, calib_clf_LR.predict_proba(CV_X), labels=clf_LR.classes_), 4)))\n",
    "\n",
    "clf_SVM = SGDClassifier(loss = \"hinge\", alpha = 10, class_weight = \"balanced\")\n",
    "clf_SVM.fit(Train_X, Train_Y)\n",
    "calib_clf_SVM = CalibratedClassifierCV(clf_SVM, method = \"sigmoid\")\n",
    "calib_clf_SVM.fit(Train_X, Train_Y)\n",
    "print(\"SVM CV Log Loss: \"+str(np.round(log_loss(CV_Y, calib_clf_SVM.predict_proba(CV_X), labels=clf_SVM.classes_), 4)))\n",
    "\n",
    "clf_RF = RandomForestClassifier(n_estimators = 1000, max_depth = 12, n_jobs = -1)\n",
    "clf_RF.fit(Train_X, Train_Y)\n",
    "calib_clf_RF = CalibratedClassifierCV(clf_RF, method = \"sigmoid\")\n",
    "calib_clf_RF.fit(Train_X, Train_Y)\n",
    "print(\"Random Forest CV Log Loss: \"+str(np.round(log_loss(CV_Y, calib_clf_RF.predict_proba(CV_X), labels=clf_RF.classes_), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_log_loss = []\n",
    "alpha = [10**x for x in range(-3, 0)]\n",
    "for i in alpha:\n",
    "    lr = SGDClassifier(loss = \"log\", alpha = i)\n",
    "    stack_clf = StackingClassifier(classifiers=[calib_clf_NB, calib_clf_LR, calib_clf_SVM, calib_clf_RF], meta_classifier=lr, use_probas=True)\n",
    "    stack_clf.fit(Train_X, Train_Y)\n",
    "    cv_log_loss.append(log_loss(CV_Y, stack_clf.predict_proba(CV_X)))\n",
    "    print(\"Stacking Classifer : For alpha value: \"+str(i)+\" Log Loss: \"+str(np.round(log_loss(CV_Y, stack_clf.predict_proba(CV_X)), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 7))\n",
    "plt.plot(alpha, cv_log_loss)\n",
    "for xy in zip(alpha, np.round(cv_log_loss, 4)):\n",
    "    plt.annotate(xy, xy)\n",
    "    \n",
    "plt.title(\"Alpha vs Log-loss\", fontsize = 20)\n",
    "plt.xlabel(\"Alpha\", fontsize = 20)\n",
    "plt.ylabel(\"Log-Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alpha[np.argmin(cv_log_loss)]\n",
    "\n",
    "lr = SGDClassifier(loss = \"log\", alpha = best_alpha)\n",
    "stack_clf = StackingClassifier(classifiers=[calib_clf_NB, calib_clf_LR, calib_clf_SVM, calib_clf_RF], meta_classifier=lr, use_probas=True)\n",
    "stack_clf.fit(Train_X, Train_Y)\n",
    "\n",
    "trainLogLoss = log_loss(Train_Y, stack_clf.predict_proba(Train_X))\n",
    "print(\"Train Log Loss on Stacking Classifier =  \"+str(np.round(trainLogLoss, 4)))\n",
    "\n",
    "cvLogLoss = log_loss(CV_Y, stack_clf.predict_proba(CV_X))\n",
    "print(\"Cross Validation Log Loss on Stacking Classifier =  \"+str(np.round(cvLogLoss, 4)))\n",
    "\n",
    "testLogLoss = log_loss(Test_Y, stack_clf.predict_proba(Test_X))\n",
    "print(\"Test Log Loss on Stacking Classifier =  \"+str(np.round(testLogLoss, 4)))\n",
    "\n",
    "print(\"Percentage of mis-classified for CV points = \"+str(np.round((np.count_nonzero(stack_clf.predict(CV_X) - CV_Y)/CV_X.shape[0]*100), 2))+\"%\")\n",
    "print(\"Percentage of mis-classified for Test points = \"+str(np.round((np.count_nonzero(stack_clf.predict(Test_X) - Test_Y)/Test_X.shape[0]*100), 2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.append(pd.DataFrame([[\"Stacking Classifier\", 0.378, 1.1311, 1.0578, \"38.98%\", \"35.09%\", \"Best Fit\"]], columns = [\"Model\", \"Train Log-loss\", \"CV Log-loss\", \"Test Log-loss\", \"Mis-Classified CV\", \"Mis-Classified Test\", \"Remarks\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusionMatrix(Test_Y, stack_clf.predict(Test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Voting Classifier: Logistic Regression, Linear SVM, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vclf = VotingClassifier(estimators=[('lr', calib_clf_LR), ('svc', calib_clf_SVM), ('rf', calib_clf_RF)], voting='soft', n_jobs = -1)\n",
    "vclf.fit(Train_X, Train_Y)\n",
    "print(\"Log loss (train) on the VotingClassifier :\"+str(np.round(log_loss(Train_Y, vclf.predict_proba(Train_X), labels=vclf.classes_), 2)))\n",
    "print(\"Log loss (CV) on the VotingClassifier :\"+str(np.round(log_loss(CV_Y, vclf.predict_proba(CV_X), labels=vclf.classes_), 2)))\n",
    "print(\"Log loss (test) on the VotingClassifier :\"+str(np.round(log_loss(Test_Y, vclf.predict_proba(Test_X), labels=vclf.classes_), 2)))\n",
    "print(\"Percentage of mis-classified for cv points :\"+str(np.round((np.count_nonzero(vclf.predict(CV_X) - CV_Y)/CV_X.shape[0]*100), 2))+\"%\")\n",
    "print(\"Percentage of mis-classified for Test points :\"+str(np.round((np.count_nonzero(vclf.predict(Test_X) - Test_Y)/Test_X.shape[0]*100), 2))+\"%\")\n",
    "print_confusionMatrix(Test_Y, vclf.predict(Test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.append(pd.DataFrame([[\"Maximum Voting Classifier\", 0.68, 1.13, 1.11, \"39.55%\", \"35.84%\", \"Best Fit\"]], columns = [\"Model\", \"Train Log-loss\", \"CV Log-loss\", \"Test Log-loss\", \"Mis-Classified CV\", \"Mis-Classified Test\", \"Remarks\"]))\n",
    "table.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = table.drop([\"Mis-Classified CV\", \"Mis-Classified Test\", \"Remarks\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2.plot(x = \"Model\", kind = \"bar\", figsize = (14, 8), grid = True, fontsize = 15)\n",
    "plt.title(\"Log losses of all the classifiers\", fontsize = 20)\n",
    "plt.ylabel(\"Log-Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>So, far our best model is Stacking Classifer with test log loss of 1.0578</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene Sequence\n",
    "\n",
    "label_headline = widgets.Label(\n",
    "                    value='Cancer Diagnosis',\n",
    "                    style={'description_width': 'initial'}\n",
    "                )\n",
    "\n",
    "geneSeq = widgets.Text(placeholder='Enter Gene Sequence Here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Button\n",
    "\n",
    "button_send = widgets.Button(\n",
    "                description='Diagnose',\n",
    "                tooltip='Click to diagnose',\n",
    "                style={'description_width': 'initial'}\n",
    "            )\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(event):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        # call gene function here\n",
    "        machineOutput = 36\n",
    "        print(f\"The gene is {machineOutput}% likely to have a cancer.\")\n",
    "\n",
    "button_send.on_click(on_button_clicked)\n",
    "\n",
    "vbox_result = widgets.VBox([button_send, output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing to mount to display\n",
    "\n",
    "vbox_headline = widgets.VBox([ label_headline])\n",
    "\n",
    "vbox_text = widgets.VBox([vbox_headline, geneSeq, vbox_result ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b5d6456c8c4b539ccc984a5cc1c5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(VBox(children=(Label(value='Cancer Diagnosis', style=DescriptionStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying Output PAge\n",
    "\n",
    "children = [vbox_text]\n",
    "tab = widgets.Tab(titles=('Slider'))\n",
    "tab.children = children\n",
    "# tab.titles = ['Cancer Diagnosis']\n",
    "tab \n",
    "\n",
    "page = widgets.HBox([vbox_headline, vbox_text])\n",
    "display(tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
